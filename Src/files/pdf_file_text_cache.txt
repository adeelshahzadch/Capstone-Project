1
Place for photoRAG
Intro
October 2025
2
2
Speaker
Maksym Lypivskyi
Head of Cloud Platforms & 
AI Director
â— 9 years at Ciklum driving large-scale cloud and 
software delivery initiatives
â— 3 years specializing in AI
â— Core interest: making AI systems reliable, 
production-ready, and business-impactful
3
3
Agenda
01
02
03
Deï¬nition & Beneï¬ts
Use cases
Limitations
4
4
The AI Adaptation Landscape
Where Does RAG Fit in Your AI Strategy?
â— Need fresh, proprietary knowledge
â— Don't want to retrain models
â— Cost-effective scaling
Our focus today
5
5
What is RAG?
Retrievalâ€‘augmented generation (RAG) is a 
pattern that augments an LLM by retrieving 
relevant information from external sources at 
query time and injecting it into the prompt.
6
6
Why RAG Matters?
â— Fresh Additional Knowledge. RAG lets 
you ground answers in recent 
documents, internal wikis, or databases 
without retraining models.
â— Better Accuracy. By retrieving 
authoritative evidence, the model 
generates more accurate answers and 
can cite sources.
â— Adaptable. Effectively handles novel and 
niche queries that weren't in the model's 
training data.
â— Increases Efï¬ciency. RAG grounds 
prompts with smaller, targeted chunks of 
information that streamline retrieval and 
generation.
7
7
RAG Pipeline Deep Dive
8
8
Chunking -  The Critical Decision
Why Chunking Matters
LLMs have limited context windows (32k-200k 
tokens). Large documents must be divided into 
smaller pieces.
The Challenge:
â— Too large â†’ loses specificity, poor retrieval
â— Too small â†’ loses context, fragmented 
information
9
9
Three Main Chunking Strategies
Strategy How It Works Pros Cons When to Use
Fixed-Size Split at 512 
tokens, 15% 
overlap
Simple, fast, 
predictable
May break 
mid-sentence
General purpose, 
starting point
Semantic Use ML to 
identify coherent 
units
Preserves 
meaning, high 
accuracy
More compute, 
slower
Technical docs, 
complex content
Recursive Split using 
separators (\n\n, 
\n, space) 
repeatedly until 
desired size
Respects 
structure, better 
boundaries
More complex 
than ï¬xed-size
Documents with 
headings, 
paragraphs
10
10
Chunking Strategy Comparison
Fixed-Size
Recursive
Semantic
11
11
Using Chunking Libraries
You Don't Need to Build from Scratch
LangChain
 LlamaIndex
Broad LLM application 
framework
Modular workï¬‚ows where 
chunking is one piece of the 
puzzle
â— Flexible TextSplitters
â— Easy integration with agents
â— Part of larger system
RAG-speciï¬c pipeline
High-performance, 
data-centric retrieval systems
â— Sophisticated NodeParsers
â— Produces optimized "Nodes"
â— Built for ingestion/retrieval
12
12
RAG real world use cases
â— AI Chatbots: RAG provides accurate answers from internal 
knowledge bases (e.g., support wikis, legal documents).  OpenAI 
emphasises that RAG is valuable when the content is not part of 
the base modelâ€™s knowledge .
â— Search & discovery: Search systems combine keyword and 
vector search to surface relevant documents in eâ€‘commerce, 
research and legal discovery.
â— AI Copilots: Tools like Supabase AI Copilots use vector 
databases to ground responses in proprietary data and maintain 
multiâ€‘tenant isolation .
â— Longâ€‘context reasoning: Databricksâ€™ longâ€‘context benchmark 
shows that Googleâ€™s Gemini 2.5 models can maintain consistent 
performance on RAG tasks up to two million tokens (longer than 
most models), whereas OpenAIâ€™s GPT 5 models achieve 
stateâ€‘ofâ€‘theâ€‘art accuracy up to 128k tokens .
13
13
Common Challenges
â— Chunking & context windows: If chunks are poorly deï¬ned, the retrieved information may miss critical context 
or include too much irrelevant text.  Research by Analytics Vidhya notes that ï¬xedâ€‘size chunking can break 
context while semanticâ€‘based chunking preserves meaning but requires more compute .
â— Model context length: Models can only ingest a ï¬nite number of tokens.  Databricksâ€™ benchmark observed that 
performance of LLMs like Llamaâ€‘3.1 and GPTâ€‘4 starts to degrade when context windows exceed 32â€“64 k tokens 
.
â— Retrieval quality: The quality of the vector database and retrieval algorithm determines recall.  Missing relevant 
documents leads to hallucinated answers.
â— Latency & cost: Large vector databases and embedding models can be expensive and introduce latency.
14
Donâ€™ts
ðŸ†‡ Rely solely on vector search
ðŸ†‡ Ignore security and access controls
ðŸ†‡ Overload the LLM context window
ðŸ†‡ Neglect continuous updates
ðŸ†‡ Skip evaluation frameworks
Doâ€™s
âœ…
 Start simple, iterate based on metrics
âœ…
 Use metadata ï¬ltering (product, language, 
permissions)
âœ…
 Combine vector + keyword search (hybrid approach)
âœ…
 Monitor retrieval quality (recall@k, precision)
âœ…
 Keep embeddings synchronized with documents
âœ…
 Evaluate with domain-speciï¬c questions
Doâ€™s and donâ€™ts for RAG
15
15
Three Main Chunking Strategies
Scenario Stack Key Reason
Learning LangChain + Chroma + 
sentence-transformers + Ollama
â— Learn fundamentals
â— Risk-free 
â— Runs on laptop
MVP LangChain + Qdrant (self-host) + 
OpenAI/Gemini embeddings + GPT-4o-mini
Professional quality at 
startup budget
Enterprise LangGraph + LangSmith + Pinecone + 
OpenAI/Gemini + GPT-4
â— Agentic workï¬‚ows
â— Observability
â— SLAs
16
âœ…
 Chunk size: 512 tokens
âœ…
 Chunk overlap: 15% (~75 tokens)
âœ…
 Top-k retrieval: 3-5 chunks
âœ…
 Embedding dimensions: 768-1536
Default Conï¬guration 
(Works for 80% of cases):
17
âœ…
 RAG = Open-book exam for AI -  Retrieves external knowledge at query time
âœ…
 Chunking is critical -  Start with 512 tokens, 15% overlap, then iterate
âœ…
 Hybrid search > vector-only -  Combine vector and keyword search
âœ…
 Start simple -  Use Chroma + LangChain for learning, scale as needed
âœ…
 Always evaluate -  Track recall, precision, and answer quality
Key Takeaways
What You Should Remember
