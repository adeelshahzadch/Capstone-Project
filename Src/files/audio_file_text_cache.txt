 session we can skip your slides. So in this session we will focus again we will reiterate a bit about the rug and we will go a bit more deeper in embeddings, vectorization and other rug systems and possibilities and we should come to the state where we can even build quite big and reliable rug system that will utilize rug. At this this slide I can skip. So we will focus on the foundation then more advanced rug patterns and some dosandons for production readiness and here we will cover even one of the questions that we had for the previous session. So on the foundation so what is in general like embeddings? So we have the problem with that example like words mesh learning algorithms, ML techniques and deep neural networks that's basically computer doesn't understand similarity of that words. Computers still understand only like numbers so it cannot help us to define just from the from the simple text are they similar or how and to find. So basically like embeddings it's a transformation of the text to the numeric representation on the vector. So when we talk about like mesh learning then for the for our embeddings we have like this representation of the numbers depends on the dimensions. So it's like on this exactly like image we have 500 36 dimensions so it's basically like graphs with the different points and in the dimension of this size we are we are putting the representation of this word on that graph. The same like for the ML algorithms, pizza and stuff like that. And if we are asking like about like mesh and learning for example then computer based on that vector representation and number representation can understand if like mesh and learning for example because we already the transformed the representation into the numbers if it's close to some of the like information that we have right now in the vector system or it's far. So based on like close and far it's basically like vector databases when we set up like choose top five results and provide into us with the answer from the five closest objects and then transform this to the text and we are basically getting that's that's text that's most closest to to the text from our request. In this case like for the mesh and learning ML algorithms is quite close because the distance 0.02 and if we just want to get only one like top element then we are getting ML algorithms pizza recipes is quite far so most probably system will not suggest this to us. And if we are going a bit like deeper to the exactly like similarity search how it works. So we have our documents we already discussed about the rock pipeline so it's already transformed to the numerical representation from the embedded models and it stores somewhere here to the vector to the vector store and we have like three documents and we fully indexed that documents and it's stored and when we have a search and if we have like user query query learn python then based on that documents that we stored our system identified the distance for our request and in this case quite close somewhat close and far away. If we set up that we want to pick up two top results so in this case we will get python tutorial and Java programming as well and then lm maybe make a decision that's to show only like python tutorial but it can be the case that it will provide as a output to the user both like python tutorial and java programming because quality of the lm play quite significant role in this case as well. Quite often like similarity metrics that systems identify this is angle between the vectors vector sorry straight line just distance between them and in some cases some dot multiplication in that systems. So why is like embedding quality matters because basically it's mean that how reliable answer will our user of our system will get and right now empty eb metrics to be honest I don't remember exact like this full abbreviation but I provided the link to the little leaderboard and you will see what exactly it's mean what's the abbreviation so this is the approach how to how current embedding models evaluated on the quality of their basically embeddings and accuracy of their embeddings and in the in the point of time when I was preparing this presentation a few of the main or like token embedding models basically from the google and token AI and one of the open source knowledge search but right now I guess chain is embedding models on a good spot there as well so you can try and play and why the exactly like embedding quality matters if we will see the different case studies or researchers you will still see that any embedding model or embedding system can provide 100% of the accuracy because still LLM or like similarity search can be like mistaken mistaken interpreter and one of the like best results that you can find like on the google website for example it's research about the legal discovery so they put 1.4 million of different low documents with utilization of Gemini embedding and they achieve 87% accuracy and it's like for sure quite good number and with another like application mind lid they achieved 82% of three top results recall in terms of their metrics it's as well quite good result and in which what is good with the direction of embedding models and we discussed a bit when we discuss about chunking strategies that's when you utilize like additional ML or GENI or embedding models it's additional time for indexing and it's additional time for the compute so it's much slower Gemini embedding already showing like quite good results in terms of the speed so they tried to vectorize 100 emails on one of the study and they achieved 21 and half seconds for vectorization of all of that emails and just to give you perspective of how it fast and what is the how fast we are moving to improving all of these tools and systems the previous result was more than 200 seconds so the order of magnitude in terms of the speed increase for the vectorization in 10 times so that system much more like improved and we are going like on that speed and that changes with many models systems and tools in AI world because people experimenting companies putting a lot of effort in this race so to say and if we will talk a bit more about the databases exactly for the vectorization this is where I wanted to mention about the speed of changes as well because early I had this slide why we need to have like separate database for embeddings and that's like current databases like systems they have different algorithms they don't have everything that is needed for the embeddings that they quiet slow for the embeddings and it was but post-griest is moving with their extension they're moving in quite rapidly and catching up the speed question and especially amount of the operations basically like queries per second that they can handle and a lot of the researchers already show that post-griest is quite good alternative to the specialized even databases for the ROG so you can see that for example like faster than pinecon and for sure like cheaper and still open source and for quadrants it can handle much more requests per second but it's on a huge amount of the data so they tested this on the 50 million embeddings and this is where like quadrants start degrading with the speed of working so potentially you can use just that database that you use to and you just need to add additional extension for the embeddings and you can continue playing with your lovely post-griest but still we have the databases for the vectorization and they still play their own game and for what use case for what systems they they they have the best results and it was already mentioned about like chroma today and chroma is quite good when you're starting some of the prototyping it's less rare used in some big production systems but they already have their cloud chroma I didn't try and still they have some kind of like limitations so chroma database quite good when you're starting and prototyping because it's quite easy to start you just pick and style chroma and almost everything is working so minimal configuration and you are starting almost like immediately with that it still has quite good additional functionality like for example metadata filtering this is the additional information to your embeddings that you can provide as an example I've already mentioned for example like category of the documents that you are feeding if you have like many of the documents and chunks for example in the metadata you can still additionally provide information about what exact document from from from from chunk is and different like multiple collection support when we are moving already like to the stage of potential MVP or already quite big production system then we need to have more reliable solution and here we can look in the direction of quadrant or for example like postgres with the PG vector extension and on the previous slide you you saw that quadrant not so good on 50 million vectors in terms of queries per second but when it's in the in the measure between like one to 10 million it's really quite quite fast and can handle a lot of the requests like for example for one million it's 626 queries per second and it's really quite fast and for that if you have that range of the vectors and you need to have quite rapid embedding quadrant is a good choice in general but if you have like billions of vectors then this is the question about another type of the system and Mewu's and they have this embedding database in a cloud it calls Zlyn it's already fully designed for quiet huge vector systems and it doesn't make sense to use this database for like much smaller systems if you have under 10 million vectors so do not recommend because that's that data and that's systems that going from the left to right it's then just harder like more complex to support setup and configure so that's why you're starting from the simplest one just for the rapid start and when your system is growing you're moving to the let's say next one and each of them just fulfill their purples about like wrap patterns wrap patterns in addition to the embeddings because we discussed about the embeddings vectorization it's not only one pattern or approach for the rock systems in addition we can have this problem with the multi hop reasoning with the connection and in the cases where we need to where we have like a bit more wake request and we need to identify the connection for that request in most cases rock traditional vector rock systems they are failing and in example that I provided and the problem that we need to have like connection built from our request that's a user asked like marketing budget compliance GDPR and basically Europe and this is the the connections and this is exactly one we need already to utilize graph system and graph rock in addition and it helps to increase reliability of our system together with LLM and quite often this is like a representation of the graph that we have and when we when our user making the request it's just like getting some of the information from the request and learning the different connections and based on the connection that we have in the rock representation it can provide much better answer if we do not have like explicit information and one chunk of the information that we are grabbing from our vector system is not enough. Main players for the graph rock solutions so now for Jay Falcon, Tiger Graph, Mamm graph and Arango DB mainly we are playing like with the now for Jay because you can easily like install it on the York laptop and I guess it's one of the most popular solution I would say. The next problem that's quite often come in the PDF processing problem so in the PDF we can have images in the PDF we can have tables and maybe some formulas so it's quiet complex documents to parse for the LLM and in the traditional rock pipelines we have like OCR objects so basically the ML system or it can be like LLM that can look exactly on the PDF and then provide in the text in the text view describe what it sees on the PDF and then we can just like vectorize it quite easily. Still we have like issues with the tables quite often and here like some OCR again can help some other approaches, captions so instead of like using complex trivial system and that's relying on OCR because they quite often like failing with that we can just use some embedding model that can understand what it's seeing so just embed the image and one of the interesting approach to solve this question for the last time it's quite like you let's say approach it's called Pali library changes so they instead of like using OCR just for the describing what information exists on the PDF and then vectorize that information they have like vision language model for that and image representation just split to the patches and then that patches basically embed to the model and when we have the retrieval then we utilize that embedding model and still like visual language model to give the answer what we have from that images and this approach shows much better results in terms of like a rock system retrieval of the information from the system and it showed better result like 15% it's approximately then standard to three-wall system with the OCR that we have. So to answer on the question quite often when we utilize the PDFs when we have a PDFs as documents we utilize the OCR for describing of that documents and then we store the information but this approach was the call Pali showing quite interesting results and maybe it's something that will be used much more often in the future as a part of this type of rock systems but still mostly OCR approach used and a bit about like agentech rock because in AI right now everything changed to the agentech and for sure this is quite interesting and quite reliable approach so in the standard rock we have query and that query go into the embedding model then we query the embedding vector databases then vector databases provide to us like candidates like top candidates and then our query so our request plus candidates that our vector database provided goes to the llem llem then process all of that information that we put and then provides to our user the answer. When we have the agentech rock we have type of like rotor agent llem and then within the two set that's available for that llem it's choose where to to roll that's request so to the rock search some external APIs or take in control over the world and then only providing the output message. Here we will have a bit more information on that so agentech patterns and mainly like query routing requests some of the composition and self correction approaches in the agentech rock system so when the user make the requests what the latest on AI regulation so llem rotor detect like latest then we will utilize the tool to road to web search so that's why I said that like in general if we are saying that we add just additional tool like web search yes it's to some extent rock system and like when we talk about the query the composition so we have a compare our Q3 to industry and predict Q4 and first of all we have the the composition of that request that's agents our llem bricks like our q3 matrix then we need to find this information in our internal database or a vector system industry q3 benchmarks potentially we don't have this information it's publicly available then web search and q4 factors again goes to the internal DB but again it's can go to the web search as well and the search pattern its self correction so we first have a retrieval then we get in the documents then our agents based on some identified our like threshold measure this and if a score is like lower than our threshold then it can even like rewrite the query and then basically like a retry again this approach with the retrieval grade docs and it can be cycled few times this self correction and when it's it's to use well when we have like why it's complex worries and in cases if everything else that we discussed earlier failed and you need to have like more high accuracy if we have one we shouldn't use this it's for the simple retrieval and if our top quality attribute for our system is latency because you understand this agentic systems where we have LLM and needs to increase the quality of the results the time of these types of the request is growing I guess we will need maybe Xanae I guess our time is and yes we are a bit out of time if you have a time you can continue and also colleagues if you have a little bit of 10 minutes you will can continue this session. I will try to finish this like in five minutes and then we will have five minutes for for the questions I just will not stop quite deeply on each of the slide just few words and what is the information important from that in addition to increase the quality of our rock systems and why I mentioned earlier that you shouldn't rely only on the vector search in addition you can utilize like best much search this is what VM25 it doesn't have semantic understanding but it's calculated the number of words that it finds like the same word that it's finding the different documents and can provide based on that statistic statistical calculation what the documents we should become and on the researchers when we have like hybrid system approach in terms of the search vector plus this VM25 it increase accuracy for a bit more than 10 percentage and DCGS3 this is like tops we recalls basically tops we candidates that we saw earlier on the diagram and when we add the rerunter in addition it's even increased to 37.2 percentage rerun can it's additional layer that we add into the system and we are grabbing from our vector system or from our database a bit more results so we are we are grabbing instead of like 510 candidates we are grabbing 100 candidates and then our rerunking system try to understand what is the best candidates for us and then provide as a result to the LLAM like 5 or 10 or maybe 3 of them and production readiness so basically what to do don't hybrid search is the best we discussed in addition about like a genetic search we discussed about the graph and where it's the most useful and what that the basis you can utilize so based on your case based on what you need to achieve you can use any of that tools metadata filtering it's quite helpful especially when you need to have like metadata in general like a story of the metadata quite helpful especially when you need to have their sources seated like for example when you make the request and get not only some information from your documents in addition the citations from what documents that information is evaluation of your system embedding three indexing and evaluation of the system quite often because you are making their updates and you can continue improving your system so don't it's like opposite of the do's so just quite helpful for you to not forget what you should do for the production systems and thank you so we have one question in our chat does embedding library I use for for my rock must match embedding library I used for training my LLAM embedding library for training your LLAM can you maybe give a bit more details yeah so before training LLAM as I understand I need to embed like all the text and everything and do the embedding yeah when you have a rock system you do not train your LLAM like fine tune your LLAM yeah it's not about I mean like when I trained my LLAM I used some embedding for example from from Facebook or Google do I have to use the same embedding library for my rock I mean as I understand like to put some data in rock you also need to do embedding do you mean when you are storing the documents to the vector database and when you are retrieving the documents from the database do you need to use the same embedding yeah so when it generates the knowledge based okay and the version and everything should be the same and identical with like with LLAM yes sometimes especially like from one provider they compatible one and another but you need to double check this information but general answer is yes good you thanks other question we have metrics like NGCG or recall require ground to answers should they come from humans this one you are preparing you are preparing in most cases yes because you are preparing the expected result on your request so yeah mainly it's human prepared metrics thank you Maxim colleagues other questions maybe you have so I have a question thank you so thank you for the session and in one of the latest slides you mentioned that we have to evaluate often and I think this is the most complex task when it comes to building the i-based solutions so my question is whether we'll have any session any session that will explain how to build this evaluations for our systems or overall like agentic systems so the question is about evaluations I will double check maybe in the next sessions that we will have on the rock enterprise productized maybe we will have a discussion on evaluation or we will discuss with our colleagues that we should potentially like add this as a part of our education because right now what what I see I don't see like exact evaluation in in the session but potentially it's a part of some of the next sessions thank you why I said not only because preparation of the data it's not so easy as well without evaluation you just don't know whether you did good job at preparing your data or not so you can be building a system for like weeks or months but if you do not have any metrics to check it's accuracy and performance then it was for nothing so as always you're one in the loop you can say you from that but yes it depends on the scalability of your system and all of that per meters yes I agree but in general what can I say it's not so easy still a question for the evaluation in general with with an award with all of these LAM systems because they are like they are not like deterministic systems and still you need to identify the proper way how you can like even identify the top three that's a system should recall so different approaches in most it must be approaches like some another LLAM utilized if you are not utilizing the people to to prepare the data so it's still not finalized question even for in general for the AI industry I would say
